{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjzWases9mVlQi5r31PVHC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omidkhalafbeigi/decision_tree_from_scratch/blob/main/Decision_Tree_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd \n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.datasets import load_iris  \n",
        "from sklearn.model_selection import train_test_split \n",
        "from collections import Counter "
      ],
      "metadata": {
        "id": "Wgl-O4fflQg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_iris()\n",
        "X, y = dataset.data, dataset.target \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "NVY2Rzk4yt-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPEp9sTgCT_R"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/luelhagos/Play-Tennis-Implementation-Using-Sklearn-Decision-Tree-Algorithm "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = load_iris()\n",
        "# X, y = dataset['data'], dataset['target']\n",
        "\n",
        "# data = {'A': ['A2', 'A2', 'A1', 'A1'], 'B': ['B1', 'B2', 'B1', 'B1'],\n",
        "#         'C': ['C2', 'C2', 'C2', 'C1'], 'D': ['D3', 'D2', 'D1', 'D1'],\n",
        "#         'E': ['E2', 'E2', 'E3', 'E1'], 'F': ['F3', 'F2', 'F4', 'F1'],\n",
        "#         'L': ['+', '+', '+', '-']}\n",
        "# dataset = pd.DataFrame(data)\n",
        "# print(dataset)\n",
        "\n",
        "# dataset = pd.read_csv('/content/Play-Tennis-Implementation-Using-Sklearn-Decision-Tree-Algorithm/Play Tennis.csv')\n",
        "# dataset = dataset.drop(labels='Day', axis=1)\n",
        "\n",
        "# data = {'sky': ['clear', 'cloudy', 'cloudy', 'clear', 'cloudy', 'cloudy', 'cloudy', 'clear'],\n",
        "#         'barometer': ['rising', 'rising', 'steady', 'falling', 'falling', 'rising', 'falling', 'rising'],\n",
        "#         'wind': ['north', 'south', 'north', 'north', 'north', 'north', 'south', 'south'],\n",
        "#         'rain': ['-', '+', '+', '-', '+', '+', '-', '-']}\n",
        "# dataset = pd.DataFrame(data)\n",
        "\n",
        "# data = {'Age': ['<=30', '<=30', '31...40', '>40', '>40', '>40', '31...40', '<=30', '<=30', '31...40', '<=30', '31...40', '31...40', '>40'],\n",
        "#         'Income': ['High', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'Medium'],\n",
        "#         'Student': ['No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No'],\n",
        "#         'Credict_rating': ['Fair', 'Excellent', 'Fair', 'Fair', 'Fair', 'Excellent', 'Excellent', 'Fair', 'Fair', 'Fair', 'Excellent', 'Excellent', 'Fair', 'Excellent'],\n",
        "#         'Buys_computer': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']}\n",
        "# dataset = pd.DataFrame(data)\n",
        "\n",
        "# data = {'Income Range': ['40-50', '30-40', '40-50', '30-40', '50-60', '20-30', '30-40', '20-30', '30-40', '30-40', '40-50', '20-30', '50-60', '40-50', '20-30'],\n",
        "#         'Credit Card Insurance': ['No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes'],\n",
        "#         'Sex': ['Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Female'],\n",
        "#         'Age': ['>43', '<=43', '<=43', '<=43', '<=43', '>43', '<=43', '<=43', '<=43', '<=43', '<=43', '<=43', '<=43', '>43', '<=43'],\n",
        "#         'Life Insurance Promotion': ['No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']}\n",
        "# dataset = pd.DataFrame(data)\n",
        "\n",
        "# number = 0 \n",
        "# dataset_dict = dict()\n",
        "\n",
        "# for column in dataset.columns:\n",
        "#   dataset_dict[number] = column\n",
        "#   number += 1 \n",
        "# columns_name = np.array(list(dataset.columns))\n",
        "\n",
        "# dataset = dataset.to_numpy()\n",
        "# X = dataset[:, :-1]\n",
        "# y = dataset[:, -1]"
      ],
      "metadata": {
        "id": "6kiPoEEZxZSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(A):\n",
        "  I = 0 \n",
        "  for uni in np.unique(A):\n",
        "    x = len(A[A == uni]) / len(A)\n",
        "    I += (x * np.log2(x))\n",
        "  I *= (-1)\n",
        "  return I "
      ],
      "metadata": {
        "id": "lo6T2uk_xbMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Two section for each feature \n",
        "\n",
        "def preprocess_data(X, y):\n",
        "  H_I = entropy(y)\n",
        "  for column_idx in range(X.shape[1]):\n",
        "    column_values = X[:, column_idx].copy()\n",
        "    uni_gain = dict()\n",
        "    for unique_value in np.unique(column_values):\n",
        "      column = column_values.copy()\n",
        "      column[column >= unique_value] = f'>={unique_value}'\n",
        "      column[column < unique_value] = f'<{unique_value}'\n",
        "      H_column = 0 \n",
        "      for uni in np.unique(column):\n",
        "        uni_idx = np.argwhere(column == uni)\n",
        "        H_column += ((len(uni_idx) / len(column)) * entropy(y[uni_idx]))\n",
        "      gain = H_I - H_column \n",
        "      uni_gain[unique_value] = gain \n",
        "\n",
        "    gain_values = list(uni_gain.values())\n",
        "    arg_gain_max = np.argmax(gain_values)\n",
        "    uni_max_gain = list(uni_gain.keys())[arg_gain_max]\n",
        "    column_values[column_values >= uni_max_gain] = f'>={uni_max_gain}'\n",
        "    column_values[column_values < uni_max_gain] = f'<{uni_max_gain}'\n",
        "    X[:, column_idx] = column_values \n",
        "    \n",
        "  return X "
      ],
      "metadata": {
        "id": "xhYB1l96xda7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Three section for each feature \n",
        "\n",
        "# def preprocess_data(X, y):\n",
        "#   H_I = entropy(y)\n",
        "#   for column_idx in range(X.shape[1]):\n",
        "#     column_values = X[:, column_idx].copy()\n",
        "#     uni_gain = dict()\n",
        "#     unique_values = np.unique(column_values)\n",
        "#     combinations = list()\n",
        "#     for idx in range(len(unique_values) - 2):\n",
        "#       combinations.append([unique_values[idx], unique_values[idx + 2]])\n",
        "#     for unique_value in combinations:\n",
        "#       column = column_values.copy()\n",
        "#       idx_lower = np.argwhere(column <= unique_value[1]).squeeze()\n",
        "#       idx_greater = np.argwhere(column >= unique_value[0]).squeeze()\n",
        "#       idx = np.intersect1d(idx_lower, idx_greater)\n",
        "#       column[idx] = '...'\n",
        "#       column[column > unique_value[0]] = f'>{unique_value[0]}'\n",
        "#       column[column < unique_value[1]] = f'<{unique_value[1]}'\n",
        "#       column[column == '...'] = f'{unique_value[0]}...{unique_value[1]}'\n",
        "#       H_column = 0 \n",
        "#       for uni in np.unique(column):\n",
        "#         uni_idx = np.argwhere(column == uni)\n",
        "#         H_column += ((len(uni_idx) / len(column)) * entropy(y[uni_idx]))\n",
        "#       gain = H_I - H_column \n",
        "#       key = f'{unique_value[0]}-{unique_value[1]}'\n",
        "#       uni_gain[key] = gain \n",
        "\n",
        "#     gain_values = list(uni_gain.values())\n",
        "#     arg_gain_max = np.argmax(gain_values)\n",
        "#     uni = list(uni_gain.keys())[arg_gain_max]\n",
        "#     uni_min, uni_max = uni.split('-')\n",
        "#     idx_lower = np.argwhere(column_values <= uni_max).squeeze()\n",
        "#     idx_greater = np.argwhere(column_values >= uni_min).squeeze()\n",
        "#     idx = np.intersect1d(idx_lower, idx_greater)\n",
        "#     column_values[idx] = '...'\n",
        "#     column_values[column_values > uni_min] = f'>{uni_min}'\n",
        "#     column_values[column_values < uni_max] = f'<{uni_max}'\n",
        "#     column_values[idx] = f'{uni_min}...{uni_max}'\n",
        "\n",
        "#     X[:, column_idx] = column_values \n",
        "    \n",
        "#   return X "
      ],
      "metadata": {
        "id": "XHTzTxzuxfbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_constant_columns(X):\n",
        "  new_X = list()\n",
        "  for column_idx in range(X.shape[1]):\n",
        "    A = X[:, column_idx]\n",
        "    split = entropy(A)\n",
        "    if split > 0:\n",
        "      new_X.append(A)\n",
        "      \n",
        "  return np.array(new_X).T"
      ],
      "metadata": {
        "id": "UlAnGRGJxhbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_iris()\n",
        "X, y = dataset['data'].astype('str'), dataset['target'].astype('str')\n",
        "X = preprocess_data(X, y) \n",
        "columns_name = list(range(X.shape[1]))\n",
        "y[y == '0'] = 'Class 0'\n",
        "y[y == '1'] = 'Class 1'\n",
        "y[y == '2'] = 'Class 2'\n",
        "X = drop_constant_columns(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
      ],
      "metadata": {
        "id": "9s5Gd52Pxjj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_root(X, y):\n",
        "  columns_gain_ratio = list()\n",
        "  H_I = entropy(y)\n",
        "  \n",
        "  for column_idx in range(X.shape[1]):\n",
        "    A = X[:, column_idx]\n",
        "    split = entropy(A)\n",
        "    H_column = 0 \n",
        "    unique_values = np.unique(A) \n",
        "    for uni in unique_values:\n",
        "      uni_idx = np.argwhere(A == uni)\n",
        "      H_column += ((len(uni_idx) / len(A)) * entropy(y[uni_idx]))\n",
        "    gain = H_I - H_column \n",
        "    gain_ratio = gain / split \n",
        "    columns_gain_ratio.append(gain_ratio)\n",
        "  root = np.argmax(columns_gain_ratio)\n",
        "\n",
        "  return root "
      ],
      "metadata": {
        "id": "R055oGF1xlfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_decision_tree(X, y, rule, rules, columns_name):\n",
        "  target_entropy = entropy(y)\n",
        "  if (target_entropy == 0) or (X.shape[1] == 0 and target_entropy > 0):\n",
        "    target = Counter(y).most_common(1)[0][0]\n",
        "    rule += str(target)\n",
        "    rules.append(rule)\n",
        "    rule = ''    \n",
        "  else:\n",
        "    root = get_root(X, y)\n",
        "    for value in np.unique(X[:, root]):\n",
        "      rule += f'{columns_name[root]} ({value}) -> ' \n",
        "      idx = np.where(X == value)[0] # Just rows \n",
        "      X_subset = X[idx].copy()\n",
        "      y_subset = y[idx].copy()\n",
        "      X_subset = np.delete(X_subset, root, axis=1) # drop specefic column \n",
        "      rules, rule = get_decision_tree(X_subset, y_subset, rule, rules, np.delete(columns_name, root, axis=0)) \n",
        "\n",
        "  return rules, rule "
      ],
      "metadata": {
        "id": "yhAWlXl7xnpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preparing_rules(rules):\n",
        "  for rule_idx in range(1, len(rules)):\n",
        "    past_rule = rules[rule_idx - 1]\n",
        "    past_rule_split = past_rule.split(' -> ')\n",
        "\n",
        "    current_rule = rules[rule_idx]\n",
        "    current_rule_split = current_rule.split(' -> ')\n",
        "\n",
        "    current_rule_first_attribute = current_rule_split[0].split(' ')[0]\n",
        "\n",
        "    past_rule_attributes = list()\n",
        "    for rule in past_rule_split:\n",
        "      rule = rule.split(' ')\n",
        "      past_rule_attributes.append(rule[0])\n",
        "\n",
        "    try:\n",
        "      idx = past_rule_attributes.index(current_rule_first_attribute)\n",
        "      found = 1 \n",
        "    except:\n",
        "      found = 0\n",
        "\n",
        "    if (idx == 0 and found == 1):\n",
        "      rules[rule_idx] = current_rule \n",
        "    elif (idx > 0 and found == 1):\n",
        "      new_rule = ''\n",
        "      for i in range(idx):\n",
        "        new_rule += past_rule_split[i]\n",
        "        new_rule += ' -> '\n",
        "      rules[rule_idx] = new_rule + current_rule \n",
        "\n",
        "  return rules"
      ],
      "metadata": {
        "id": "J3SSrbhPxpmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules = list()\n",
        "rule = ''\n",
        "rules, _ = get_decision_tree(X_train, y_train, rule, rules, columns_name)\n",
        "rules = preparing_rules(rules) "
      ],
      "metadata": {
        "id": "9sBaw-Obxxrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql0V0OiqxzG-",
        "outputId": "5cdfdfff-e999-447e-927f-5d020571381d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2 (<3.0) -> Class 0',\n",
              " '2 (>=3.0) -> 3 (>=1.0) -> 0 (<5.6) -> 1 (<3.4) -> Class 1',\n",
              " '2 (>=3.0) -> 3 (>=1.0) -> 0 (>=5.6) -> 1 (<3.4) -> Class 2',\n",
              " '2 (>=3.0) -> 3 (>=1.0) -> 0 (>=5.6) -> 1 (>=3.4) -> Class 2']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rules_len = [len(rule) for rule in rules]\n",
        "max_len = np.argmax(rules)\n",
        "selected_rule = rules[max_len]\n",
        "order = [order.split(' ')[0] for order in [element.strip() for element in selected_rule.split('->')]]\n",
        "del order[-1]\n",
        "order = np.array(order).astype(np.int16)\n",
        "X_test = X_test[:, order]"
      ],
      "metadata": {
        "id": "K-d2qQIWx089"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules_list = list()\n",
        "for rule in rules:\n",
        "  rule_list = list()\n",
        "  rule_slice = rule.split(' -> ')\n",
        "  for r in rule_slice:\n",
        "    if not r.startswith('Class'):\n",
        "      key = r.split(' ')[1]\n",
        "      rule_list.append(key.replace('(', '').replace(')', ''))\n",
        "    else:\n",
        "      rule_list.append(r.replace('(', '').replace(')', ''))\n",
        "  rules_list.append(rule_list)"
      ],
      "metadata": {
        "id": "-2R9I8ziQnU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules_list "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn1AxQVhUt3Q",
        "outputId": "aab96600-401c-4464-eefe-16ed272c6a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<3.0', 'Class 0'],\n",
              " ['>=3.0', '>=1.0', '<5.6', '<3.4', 'Class 1'],\n",
              " ['>=3.0', '>=1.0', '>=5.6', '<3.4', 'Class 2'],\n",
              " ['>=3.0', '>=1.0', '>=5.6', '>=3.4', 'Class 2']]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = list()\n",
        "for sample in X_test:\n",
        "  for rule in rules_list:\n",
        "    for feature_idx in range(len(rule)):\n",
        "      rule_feature = rule[feature_idx]\n",
        "      if not rule_feature.startswith('Class'):\n",
        "        sample_feature = sample[feature_idx]\n",
        "        if sample_feature == rule_feature: continue \n",
        "        else: break \n",
        "      else:\n",
        "        pred.append(rule_feature)\n",
        "        \n",
        "dt_accuracy = accuracy_score(y_test, pred)\n",
        "print(f'DT Accuracy: {dt_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gd4emEZVi_i",
        "outputId": "4c57e932-7451-4af5-c383-b8c3383f3d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DT Accuracy: 0.8\n"
          ]
        }
      ]
    }
  ]
}